# -*- coding: utf-8 -*-
"""RLtoywgitFil4Z.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAmKzEtKsUDkPiAbvsz27_n3VKypNpOp

# Imports
"""

#!wget https://raw.githubusercontent.com/anton-baykalov/toytoric/main/toy_setup.py

"""import bisect
import numpy as np
import sympy as sp
import IPython
import random
import copy
import math
import torch
from torch import nn

CPU or GPU

if torch.cuda.is_available():
    Dvc = torch.device("cuda:0")
    print("Running on GPU cuda:0")
else:
    Dvc = torch.device("cpu")
    print("Running on CPU")

CpuDvc = torch.device("cpu")
"
"""


"""# Setting up the initial state

In this example, it's subalgebras for $\mathrm{Fil}_4 \oplus \mathbb{Z}$.


"""

# %% imports

from toy_setup_sage import *

# %% getting toric datum for Fil4 and setting up the polynomial ring
L = Zeta.lookup('Fil4') 
t = L.toric_datum('subalgebras')
#R = t.ring #R = PolynomialRing(QQ, 'x', 15)
#R.inject_variables()
#L


# %% making multiplication table for Fil4 * Z
MTF4=L.table
MTF4
MT = [[[0 for k in range(6)] for j in range(6)] for i in range(6)]
for i in range(5):
    for j in range(5):
        for k in range(5):
            MT[i][j][k] = MTF4[i][j][k]
print(MTF4)
print(MT)

# %% 
LZ = Zeta.Algebra(MT)
TZ = LZ.toric_datum('subalgebras')
#R = TZ.ring #R = PolynomialRing(QQ, 'x', 15)
#R.inject_variables()

print(TZ)


# %% pull the polyhedron into cone conditions
T0 = pol_to_cc_td(TZ)
print(T0)






# %%
"""
r = 6
n = r*(r+1)//2

gens = list(sp.symbols('x0:{}'.format(n)))

# Define individual symbols for direct use in expressions
for sym in gens:
    globals()[sym.name] = sym

cc_symbols = [(x15*x18, x1*x11*x15 + x0*x12*x15 - x0*x11*x16), (x11*x15, x0*x7*x11 - x0*x6*x12),
              (x15*x18*x20, x0*x11*x17*x18 + x1*x11 *
               x15*x19 + x0*x12*x15*x19 - x0*x11*x16*x19),
              (x11*x15*x18, x2*x6*x11*x15 - x1*x7*x11*x15 - x0*x8 *
               x11*x15 + x0*x6*x13*x15 + x0*x7*x11*x16 - x0*x6*x12*x16),
              (x11*x15*x18*x20, x0*x6*x14*x15*x18 + x0*x7*x11*x17*x18 - x0*x6*x12*x17*x18 - x2*x6*x11*x15*x19 +
               x1*x7*x11*x15*x19 + x0*x8*x11*x15*x19 - x0*x6*x13*x15*x19 - x0*x7*x11*x16*x19 + x0*x6*x12*x16*x19),
              (x18, x6*x11), (x18*x20, x6*x11*x19), (x18, x0*x15), (x18*x20, x0*x15*x19), (x15, x0*x11), (x11, x0*x6)]


cc = [(sp.Poly(p1, *gens, domain=sp.ZZ), sp.Poly(p2, *gens, domain=sp.ZZ))
      for p1, p2 in cc_symbols]
"""

# %%
"""


# NN Agents and reinforcement




### In the cell below:

Constructor for an NN that takes in (state, turn) where state is output of `ToyState.code_for_torch` and move is seqwence of zeroes and one 1 of the place with the index = number of the turn. Returns a tensor of shape (1, M_length) which can be softmaxed and then converted to input for `code_to_move1`. This will produce a move that is either a blowup or a change of variables.

**I DO NOT CURRENTLY USE THIS VERSION BELOW**

since there are way to many moves that do not do anything since thay are illigal changes of variables. I now use the version that is below this one. There, the moves are only blowups and between moves I use function `trick_changevar_reduce` from `toy_setup.py` that runs through cone conditions and tries to find good changes of variables and does them if this improves the score.


"""

# Defining the initial state
# and constants for our game

r = 6
n = r*(r+1)//2
num_of_cond = 11  # number of conditions in cc
# maximal allowed number of terms in a condition (game will ignore moves that increase numbr of terms above this)
max_terms = 12
# maximal allowed degree of a term in a polynomial not needed for imput of the agent (state.data_for_torch())
max_degree = 7
# maximum degree of a change of variables (degree of P in "x ->x' +P")
change_max_degree = 2
N = 10  # max lenght of state (number of ToyTorics is it)

num_of_coef = 2  # if over Z, only +- 1
# num_of_coef = 32002 # if over finite field

#  length of a torch tensor incoding a state (first input of agent)
# if no_coef is False (we want to feed coefficient to NN)
S_length = N*(num_of_cond*(n+(n+1)*max_terms))
# S_length = N*(num_of_cond*(n+(n)*max_terms)) # if no_coef is True (we only feed exponents to NN, we forget coefficients)

n_turns = 10  # number of turns we want to play (second imput of agent)

# length of a torch tensor incoding a move (output of agent)
M_length = 1+N*(math.comb(n, 2)+n*num_of_coef *
                (math.comb(n+change_max_degree, change_max_degree)-1))

no_coef = False  # coefficients are recorded in the data of the state

# creating generators x0, x1,... so on
#gens = list(sp.symbols('x0:{}'.format(n)))
T = T0

R = T.ring #R = PolynomialRing(QQ, 'x', 15)
R.inject_variables()
gens = R.gens()

# this will create a standard integrand for the initial state
#integr = random_cc(6, 6, 6, 4, None).integrand
# creating ToyToric datum using cone conditions for L_{6,14}
#toyt = ToyToric(integr, cc, None, gens)


S = ToyState(T, N)  # Starting state


class FCModel(nn.Module):
    def __init__(self, layer_dims):
        super().__init__()

        # Deal with the state and one-hot move vectors separately, since we can do this efficiently.
        self.first_state = nn.Linear(S_length, layer_dims[0])
        self.first_move = nn.Linear(n_turns, layer_dims[0], bias=False)

        self.layers = nn.ModuleList([
            nn.Linear(a, b)
            for a, b in zip(layer_dims, layer_dims[1:])
        ])

        self.fully_conn = nn.Linear(layer_dims[-1], M_length)

    def forward(self, state, move):
        x = self.first_state(state) + self.first_move(move)
        x = nn.functional.relu(x)

        for linear in self.layers:
            x = linear(x)
            x = nn.functional.relu(x)

        x = self.fully_conn(x)
        x = torch.softmax(x, dim=1)
        return x


# Shape (1, S_length): the first index is the batch number.
x = torch.ones((1, S_length))
# Shape (1, n_turns): the first index is the batch number.
y = torch.ones((1, n_turns))
# Test that we get back something of shape (1, 1).
out = FCModel([10, 10])(x, y)
out.softmax(dim=1)
max_index = torch.argmax(out)
print("Index of max value:", max_index)

one_hot_tensor_direct = torch.nn.functional.one_hot(
    max_index.unsqueeze(0), num_classes=M_length).float()
one_hot_tensor_direct
torch.sum(one_hot_tensor_direct)

print("Input lenght S_length", S_length)
print("Output lenght M_length", M_length)

print(T)
reduce_td(T)
print(T)
type(T.cc)

# %%

"""# Models for games with blowups only

1. Here we do not need to incode coefficients into data for the model since we only choose blowups.

2. The output move is $1+ \binom{n}{2}$ buckets: do nothing + blowups.


"""

r = 6
n = r*(r+1)//2
num_of_cond = 11
max_terms = 12
max_degree = 7  # not needed for imput of the agent (state.data_for_torch())
# change_max_degree = 2 # maximum degree of a change of variables
N = 10  # max lenght of state (number of ToyTorics is it)

# num_of_coef =2 #if over Z, only +- 1
# num_of_coef = 32002 # if over finite field

#  length of a torch tensor incoding a state (first input of agent)
# S_length = N*(num_of_cond*(n+(n+1)*max_terms)) # if no_coef is False
S_length = N*(num_of_cond*(n+(n)*max_terms))  # if no_coef is True

n_turns = 10  # number of turns we want to play (second imput of agent)

# length of a torch tensor incoding a move (output of agent)
M_length = 1+N*(math.comb(n, 2))

no_coef = True  # no coefficients recorded in the data of the state

# creating generators x0, x1,... so on
#gens = list(sp.symbols('x0:{}'.format(n)))
T = T0

R = T.ring #R = PolynomialRing(QQ, 'x', 15)
R.inject_variables()
gens = R.gens()

# this will create a standard integrand for the initial state
#integr = random_cc(6, 6, 6, 4, None).integrand
# creating ToyToric datum using cone conditions for L_{6,14}
#toyt = ToyToric(integr, cc, None, gens)


S = ToyState(T, N)  # Starting state


class ModelBU(nn.Module):
    def __init__(self, layer_dims):
        super().__init__()

        # Deal with the state and one-hot move vectors separately, since we can do this efficiently.
        self.first_state = nn.Linear(S_length, layer_dims[0])
        self.first_move = nn.Linear(n_turns, layer_dims[0], bias=False)

        self.layers = nn.ModuleList([
            nn.Linear(a, b)
            for a, b in zip(layer_dims, layer_dims[1:])
        ])

        self.fully_conn = nn.Linear(layer_dims[-1], M_length)

    def forward(self, state, move):
        x = self.first_state(state) + self.first_move(move)
        x = nn.functional.relu(x)

        for linear in self.layers:
            x = linear(x)
            x = nn.functional.relu(x)

        x = self.fully_conn(x)
        x = torch.softmax(x, dim=1)
        return x


# Shape (1, S_length): the first index is the batch number.
x = torch.ones((1, S_length))
# Shape (1, n_turns): the first index is the batch number.
y = torch.ones((1, n_turns))
# Test that we get back something of shape (1, 1).
out = FCModel([10, 10])(x, y)
out.softmax(dim=1)
max_index = torch.argmax(out)
print("Index of max value:", max_index)

one_hot_tensor_direct = torch.nn.functional.one_hot(
    max_index.unsqueeze(0), num_classes=M_length).float()
one_hot_tensor_direct
torch.sum(one_hot_tensor_direct)

print("Input lenght S_length", S_length)
print("Output lenght M_length", M_length)





# %% Playing games

# to play games with an agent
def play_games(agent, k_games, start_state, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree):
    """
    Play k_games starting at start_state, using the agent to give probability distributions on each move.
    """
    # bunch_of_states = [copy.deepcopy(start_state) for _ in range(k_games)] # Create k_games copies of the start_state
    # Create a numpy array of k_games copies of the start_state
    bunch_of_states = np.empty(k_games, dtype=object)
    for i in range(k_games):
        bunch_of_states[i] = copy.deepcopy(start_state)

    stateid = (start_state.get_data_torch(
        num_of_cond, max_terms, no_coef)).repeat(k_games, 1)
    moveid = torch.eye(n_turns)

    actions_record = np.zeros((k_games, n_turns, M_length))
    stateid_record = np.zeros((k_games, n_turns, S_length))

    for step in range(n_turns):
        # print("Turn:", step)
        with torch.no_grad():
            # Agent outputs probabilities/scores for each move for the batch of states
            move_output = agent(stateid, moveid[step].unsqueeze(
                0).repeat(k_games, 1))  # Shape (k_games, M_length)

        # Determine the chosen move for each game in the batch
        # We take the argmax along the M_length dimension (dimension 1)
        # chosen_moves_indices = torch.argmax(move_output, dim=1) # Shape (k_games,)

        for i in range(k_games):
            # Get the chosen move index for the current game (i)
            # chosen_move_index = chosen_moves_indices[i].item()
            stateid_record[i][step] = stateid[i].numpy()

            move_np = move_output[i].numpy()
            max_ind = np.argmax(move_np)
            rand_move = np.random.rand(M_length)

            # some probability for a random move instead
            if np.random.rand(M_length)[max_ind] < move_np[max_ind]:
                # Apply the chosen move to the corresponding state
                actions_record[i][step] = move_np
                bunch_of_states[i].code_to_move1(
                    move_np, N, change_max_degree, max_terms, max_degree)
            else:
                actions_record[i][step] = rand_move
                bunch_of_states[i].code_to_move1(
                    rand_move, N, change_max_degree, max_terms, max_degree)

            # Update the state representation for the next turn

            stateid[i] = bunch_of_states[i].get_data_torch(
                num_of_cond, max_terms, no_coef)

            # probably need to record the moves, as it is impossible to recover them from the final states
    return bunch_of_states, actions_record, stateid_record


def play_random_games(agent, k_games, start_state, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree):  # to play random games
    """
    Play k_games starting at start_state, using the agent to give probability distributions on each move.
    """
    # bunch_of_states = [copy.deepcopy(start_state) for _ in range(k_games)] # Create k_games copies of the start_state
    # Create a numpy array of k_games copies of the start_state
    bunch_of_states = np.empty(k_games, dtype=object)
    for i in range(k_games):
        bunch_of_states[i] = copy.deepcopy(start_state)

    stateid = (start_state.get_data_torch(
        num_of_cond, max_terms, no_coef)).repeat(k_games, 1)
    moveid = torch.eye(n_turns)

    actions_record = np.zeros((k_games, n_turns, M_length))
    stateid_record = np.zeros((k_games, n_turns, S_length))

    for step in range(n_turns):
        # Determine the chosen move for each game in the batch
        # We take the argmax along the M_length dimension (dimension 1)
        # chosen_moves_indices = torch.argmax(move_output, dim=1) # Shape (k_games,)

        for i in range(k_games):
            # Get the chosen move index for the current game (i)
            # chosen_move_index = chosen_moves_indices[i].item()
            stateid_record[i][step] = stateid[i].numpy()

            rand_move = np.random.rand(M_length)

            actions_record[i][step] = rand_move
            bunch_of_states[i].code_to_move1(
                rand_move, N, change_max_degree, max_terms, max_degree)

            # Update the state representation for the next turn

            stateid[i] = bunch_of_states[i].get_data_torch(
                num_of_cond, max_terms, no_coef)

            # probably need to record the moves, as it is impossible to recover them from the final states
    return bunch_of_states, actions_record, stateid_record

# Check that we can generate games from a model.
# Pass the necessary arguments to play_games
# games, actions, states = play_games(FCModel([100, 100]), 100, S, num_of_cond, max_terms, False, N, change_max_degree, max_degree)


"""print(games[1])
print((actions[1][1]).shape)
print((states[1][1]).shape)
ngames= states.shape
ngames

states.shape
states_torch = torch.from_numpy(states)
states_torch.shape
states_torch = states_torch.flatten(end_dim=1)
states_torch.shape[1]
states.shape[1]



# Reinforcing games
"""
# %% Play one game

def play_one_game(agent, start_state, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree):
    """
    Play k_games starting at start_state, using the agent to give probability distributions on each move.
    """
    # bunch_of_states = [copy.deepcopy(start_state) for _ in range(k_games)] # Create k_games copies of the start_state
    # Create a numpy array of k_games copies of the start_state
    one_game_state = np.empty(1, dtype=object)
    
    one_game_state[0] = copy.deepcopy(start_state)

    stateid = (start_state.get_data_torch(
        num_of_cond, max_terms, no_coef)).repeat(1, 1)
    moveid = torch.eye(n_turns)

    actions_record = np.zeros((1, n_turns, M_length))
    stateid_record = np.zeros((1, n_turns, S_length))

    for step in range(n_turns):
        # print("Turn:", step)
        with torch.no_grad():
            # Agent outputs probabilities/scores for each move for the batch of states
            move_output = agent(stateid, moveid[step].unsqueeze(
                0).repeat(1, 1))  # Shape (k_games, M_length)

        # Determine the chosen move for each game in the batch
        # We take the argmax along the M_length dimension (dimension 1)
        # chosen_moves_indices = torch.argmax(move_output, dim=1) # Shape (k_games,)

        
        # Get the chosen move index for the current game (i)
        # chosen_move_index = chosen_moves_indices[i].item()
        stateid_record[0][step] = stateid[0].numpy()

        move_np = move_output[0].numpy()
        max_ind = np.argmax(move_np)
        rand_move = np.random.rand(M_length)

    
        # Apply the chosen move to the corresponding state
        actions_record[0][step] = move_np
        one_game_state[0].code_to_move1(
            move_np, N, change_max_degree, max_terms, max_degree)
    
        # Update the state representation for the next turn

        stateid[0] = one_game_state[0].get_data_torch(
            num_of_cond, max_terms, no_coef)

        # probably need to record the moves, as it is impossible to recover them from the final states
    return one_game_state, actions_record, stateid_record


# %%


def reinforce_games(
    model,
    optimiser,
    # games,
    np_states,
    np_moves,
    batch_size=100,
):
    """
    Given a list of completed games, reinforce each move in each game.
    """
    # n_games, _ = games.shape
    # have to be equal to batch_size? actually, no. It is a batch we reinforcing
    n_games = np_states.shape[0]
    # so batch_size ideally need to divide n_games
    n_turns = np_states.shape[1]
    S = np_states.shape[2]
    M = np_moves.shape[2]
    # stateid = torch.zeros((n_games, n_turns, S_length))
    # moveid = torch.eye(n_turns)

    # Unpack the games into (state, move, actions).
    # I don't think I need this stage as everything is already unpacked
    # states = torch.zeros((n_games, n_nurns, S))
    moveids = torch.zeros((n_games, n_turns, n_turns))
    # actions = torch.zeros((n_games, n_turns, M))
    for t in range(n_turns):
        moveids[:, t, t] = 1
        # states[:, t+1:, t] = torch.from_numpy(games[:, None, t])
        # actions[:, t, 0] = torch.from_numpy(games[:, t])

    # Reshape these so that we can shuffle the moves between games.
    states = torch.from_numpy(np_states).float()
    states = states.flatten(end_dim=1)
    moveids = moveids.flatten(end_dim=1)
    actions = torch.from_numpy(np_moves).float()
    actions = actions.flatten(end_dim=1)

    # Reinforce
    criterion = nn.CrossEntropyLoss()
    shuffle = torch.randperm(n_games * n_turns)
    for i in range(0, n_games * n_turns, batch_size):
        batch = shuffle[i:i+batch_size]

        optimiser.zero_grad()
        predicted = model(states[batch], moveids[batch])
        loss = criterion(predicted, actions[batch])
        loss.backward()
        optimiser.step()


# Check that this works.
"""
model = FCModel([100, 100])
reinforce_games(
    model=model,
    optimiser=torch.optim.SGD(model.parameters(), lr=0.01),
    np_states = states,
    np_moves = actions,
)
"""

# %%

K_GAMES = 400
KEEP_TOP = K_GAMES//10

model = ModelBU([200, 100, 100])
optimiser = torch.optim.SGD(model.parameters(), lr=0.1)

# Initiating top games storage

# Keep only the top so many games here.
top_states = np.zeros((0, n_turns, S_length), dtype=np.float32)
top_actions = np.zeros((0, n_turns, M_length), dtype=np.float32)

# Initialize top_games as an empty numpy array of objects
top_games = np.array([], dtype=object)

# %% Playing random games to fing good ones

for i in range(5):
    # Play some new games.
    new_games, new_actions, new_states = play_random_games(
        model, K_GAMES, S, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree)

    # Mix these games into our pot of the best games so far.
    # Concatenate top_games (numpy array) and new_games (list, which np.concatenate handles)
    all_games = np.concatenate([top_games, new_games], axis=0)
    actions = np.concatenate([top_actions, new_actions], axis=0)
    states = np.concatenate([top_states, new_states], axis=0)

    # Score the games, and rearrange into descending order by score
    scores = np.array([-game.weight() for game in all_games])
    order = np.argsort(scores)[-KEEP_TOP:][::-1]
    top_games = all_games[order]  # top_games remains a numpy array
    top_actions = actions[order]
    top_states = states[order]

    # Train or comment out and do not train so we accomulate good solutions first
    # reinforce_games(model, optimiser, top_states, top_actions, batch_size=20)

    # Display progress

    print("itiration:", i, "min weight:", top_games[0].weight())

game0 = top_games[0]
print(S.weight())
print(game0.weight())
print(S)
print(game0)

for i in range(10):
    print("move:", i)
    move1_uncode(gens, top_actions[0][i], N,
                 change_max_degree, max_terms, max_degree)
    print("\n")

# %% This cell can be re-run without resetting the model.

# Keep only the top so many games here.
# top_states = np.zeros((0, n_turns, S_length), dtype=np.float32)
# top_actions = np.zeros((0, n_turns, M_length), dtype=np.float32)

# Initialize top_games as an empty numpy array of objects
# top_games = np.array([], dtype=object)


for i in range(40):
    # Play some new games.
    new_games, new_actions, new_states = play_games(
        model, K_GAMES, S, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree)

    # Mix these games into our pot of the best games so far.
    # Concatenate top_games (numpy array) and new_games (list, which np.concatenate handles)
    all_games = np.concatenate([top_games, new_games], axis=0)
    actions = np.concatenate([top_actions, new_actions], axis=0)
    states = np.concatenate([top_states, new_states], axis=0)

    # Score the games, and rearrange into descending order by score
    scores = np.array([-game.weight() for game in all_games])
    order = np.argsort(scores)[-KEEP_TOP:][::-1]
    top_games = all_games[order]  # top_games remains a numpy array
    top_actions = actions[order]
    top_states = states[order]

    # Train
    reinforce_games(model, optimiser, top_states, top_actions, batch_size=20)

    # Display progress

    print("itiration:", i, "min weight:", top_games[0].weight(), "lenght:", top_games[0].length)

game0 = top_games[0]
print(S.weight())
print(game0.weight())
print(S)
print(game0)

# %%

for i in range(10):
    print("move:", i)
    move1_uncode(gens, top_actions[0][i], N,
                 change_max_degree, max_terms, max_degree)
    print("\n")


# %% play a single game
one_game, one_actions, one_states = play_games(
    model, 1, S, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree)

# %% play a single game 2
one_game, one_actions, one_states = play_one_game(
    model, S, num_of_cond, max_terms, no_coef, N, change_max_degree, max_degree)

# %% best output so far
'''
ToyState:
  Length: 4
  Torics:
    -       Integrand: [array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1]), array([ 0, -1, -1, -1, -1, -1,      1, -1, -1, -1, -1,      2,      1, -1, -1,      3, -1,
                   -1,      4, -1,      5])]
      Cone Conditions: [
                  (x15*x18, -x0*x11*x12*x16 + x0*x12*x15 + x1*x11*x12*x15),
                  (x11*x15, x0*x11*x7 - x0*x6),
                  (x15*x18*x20, -x0*x11*x12*x16*x19 + x0*x11*x12*x17*x18 + x0*x12*x15*x19 + x1*x11*x12*x15*x19),
                  (x11*x12*x15*x18, -x0*x11*x12*x15*x8 + x0*x11*x12*x16*x7 - x0*x12*x16*x6 + x0*x13*x15*x6 - x1*x11*x12*x15*x7 + x11*x12*x15*x2*x6),
                  (x11*x12*x15*x18*x20, x0*x11*x12*x15*x19*x8 - x0*x11*x12*x16*x19*x7 + x0*x11*x12*x17*x18*x7 + x0*x12*x16*x19*x6 - x0*x12*x17*x18*x6 - x0*x13*x15*x19*x6 + x0*x14*x15*x18*x6 + x1*x11*x12*x15*x19*x7 - x11*x12*x15*x19*x2*x6),
                  (x18, x11*x12*x6),
                  (x18*x20, x11*x12*x19*x6),
                  (x18, x0*x15),
                  (x18*x20, x0*x15*x19),
                  (x15, x0*x11*x12),
                  (x11*x12, x0*x6)
            ]
      Polynomial Ring: None
      Variables: [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20]
    -       Integrand: [array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1]), array([ 0, -1, -1, -1, -1, -1,      1, -1, -1, -1, -1,      0, -1, -1, -1,      3, -1,
                   -1,      4, -1,      5])]
      Cone Conditions: [
                  (x18, x1*x11),
                  (1, 0),
                  (x18*x20, x0*x11*x17*x18 + x1*x11*x19),
                  (x11*x18, -x0*x11*x15*x8 + x0*x13*x15*x6 - x1*x11*x7 + x11*x2*x6),
                  (x11*x18*x20, x0*x11*x15*x19*x8 + x0*x11*x17*x18*x7 - x0*x13*x15*x19*x6 + x0*x14*x15*x18*x6 + x1*x11*x19*x7 - x11*x19*x2*x6),
                  (x18, x11*x6),
                  (x18*x20, x11*x19*x6),
                  (x18, x0*x15**2),
                  (x18*x20, x0*x15**2*x19),
                  (1, 0),
                  (x11, x0*x15*x6)
            ]
      Polynomial Ring: None
      Variables: [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20]
    -       Integrand: [array([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1]), array([ 0, -1, -1, -1,      0, -1,      1, -1, -1, -1, -1,      1, -1, -1, -1,      3, -1,
                   -1,      4, -1,      5])]
      Cone Conditions: [
                  (x18, x1*x11*x4),
                  (1, 0),
                  (x18*x20, x0*x11*x17*x18*x4 + x1*x11*x19*x4),
                  (x11*x18*x4, -x0*x11*x15*x4*x8 + x0*x13*x15*x6 - x1*x11*x4*x7 + x11*x2*x4*x6),
                  (x11*x18*x20*x4, x0*x11*x15*x19*x4*x8 + x0*x11*x17*x18*x4*x7 - x0*x13*x15*x19*x6 + x0*x14*x15*x18*x6 + x1*x11*x19*x4*x7 - x11*x19*x2*x4*x6),
                  (x18, x11*x4*x6),
                  (x18*x20, x11*x19*x4*x6),
                  (x18, x0*x15**2),
                  (x18*x20, x0*x15**2*x19),
                  (1, 0),
                  (x11*x4, x0*x15*x6)
            ]
      Polynomial Ring: None
      Variables: [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20]
    -       Integrand: [array([2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]), array([ 3, -1, -1, -1, -1, -1,      1, -1, -1, -1, -1,      1, -1, -1, -1,      3, -1,
                   -1,      4, -1,      5])]
      Cone Conditions: [
                  (x15*x18, -x11*x16),
                  (x15, x7),
                  (x15*x18*x20, -x11*x16*x19 + x11*x17*x18),
                  (x11*x15*x18, -x0*x11*x15*x8 + x0*x13*x15*x6 + x11*x15*x2*x6 + x11*x16*x7),
                  (x11*x15*x18*x20, x0*x11*x15*x19*x8 - x0*x13*x15*x19*x6 + x0*x14*x15*x18*x6 - x11*x15*x19*x2*x6 - x11*x16*x19*x7 + x11*x17*x18*x7),
                  (x18, x11*x6),
                  (x18*x20, x11*x19*x6),
                  (x18, x0**2*x15),
                  (x18*x20, x0**2*x15*x19),
                  (x15, x11),
                  (x11, x0*x6)
            ]
      Polynomial Ring: None
      Variables: [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20]


move: 0
cc_ind: 2
blowup: x5 - x13


move: 1
cc_ind: 2
blowup: x7 - x17


move: 2
cc_ind: 3
blowup: x4 - x11


move: 3
cc_ind: 3
blowup: x8 - x17


move: 4
cc_ind: 4
blowup: x17 - x19


move: 5
cc_ind: 1
blowup: x3 - x17


move: 6
cc_ind: 0
blowup: x11 - x12


move: 7
cc_ind: 1
blowup: x0 - x15


move: 8
cc_ind: 4
blowup: x2 - x11


move: 9
cc_ind: 1
blowup: x4 - x11

'''